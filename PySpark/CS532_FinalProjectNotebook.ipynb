{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 532 - Systems For Data Science - Final Project Colab Notebook\n",
        "\n",
        "Link to this notebook:\n",
        "\n",
        "https://colab.research.google.com/drive/1KgIYZMCAQnDeVJ0IbXASlONajhT6oBSw?usp=sharing\n",
        "\n",
        "Link to the containing Google Drive folder:\n",
        "\n",
        "https://drive.google.com/drive/folders/14Y9p6RUPPtwTbFi_WvHVOiVapHydLt9f?usp=sharing\n",
        "\n",
        "Link to the GitHub repository (which contains a README):\n",
        "\n",
        "https://github.com/matdjohnson-at-umass-dot-edu/CS532-FinalProject\n"
      ],
      "metadata": {
        "id": "Qpv20Hkf0Z6j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkKBJbLHf8Te",
        "outputId": "400b2161-2f5e-46a9-a4ad-99ab86ae2da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ir-datasets\n",
            "  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.13.4)\n",
            "Collecting inscriptis>=2.2.0 (from ir-datasets)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting lxml>=4.5.2 (from ir-datasets)\n",
            "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.67.1)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir-datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir-datasets)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir-datasets)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir-datasets)\n",
            "  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting ijson>=3.1.3 (from ir-datasets)\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir-datasets)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (20.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2025.4.26)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18995 sha256=6c6cf7f318ca56141839cc2c3717dd530e5a53e1d4a6ffa6790dd3bc6cbf5ae9\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
            "  Building wheel for cbor (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53975 sha256=54a384b5d3a42f78bbe3a1d31c632aab4ad46d7c29d5ba82435c089dea81f1e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, lz4, lxml, inscriptis, ir-datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.3.0 inscriptis-2.6.0 ir-datasets-0.5.10 lxml-5.4.0 lz4-4.4.4 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.9\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.5-py2.py3-none-any.whl size=317747923 sha256=d03e84f73cfe956c1cf40c00ade3d8feee6a0d535d43d3f82d94f635b7eb2ec6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/7f/b4/0e68c6d8d89d2e582e5498ad88616c16d7c19028680e9d3840\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.5\n",
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.3.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "! pip install ir-datasets\n",
        "! pip install pyspark\n",
        "! pip install mysql-connector-python\n",
        "\n",
        "# ! apt-get update\n",
        "# ! dpkg --configure -a\n",
        "# ! apt-get install mysql-server\n",
        "# ! /etc/init.d/mysql start\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ! mysql --user=root < /content/drive/MyDrive/CS532-FinalProject/config/init.sql\n",
        "# ! echo \"localhost  \" >> /etc/hosts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBOo7yQ3lxxu",
        "outputId": "80eb1529-d80c-4cb4-d6ac-056bc55dfd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"96cm-1ec-96ei-3400em-2200epm\", \"index_map_execution_time\", \"6.12798810005188\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('it', [['1781133', '1']]), ('was', [['1781133', '1']]), ('used', [['1781133', '2']]), ('in', [['1781133', '6']]), ('landing', [['1781133', '1']]), ('craft', [['1781133', '1']]), ('during', [['1781133', '1']]), ('world', [['1781133', '1']]), ('war', [['1781133', '1']]), ('ii', [['1781133', '1']])]\n",
            "\"96cm-1ec-96ei-3400em-2200epm\", \"index_reduce_execution_time\", \"51.60756802558899\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('patrician', [['89775', '1'], ['966188', '1'], ['690474', '1'], ['835096', '1'], ['485731', '1'], ['1542437', '1'], ['1588445', '1'], ['913061', '1'], ['277013', '1'], ['2313490', '1'], ['568664', '1'], ['1552796', '1'], ['189590', '1'], ['2332339', '1'], ['1096814', '1'], ['1721217', '1'], ['1817793', '1'], ['1117392', '1'], ['632533', '2'], ['830970', '1'], ['913106', '1'], ['1472487', '2'], ['1668233', '1'], ['19620', '1'], ['1975952', '1'], ['1496175', '1'], ['73382', '1'], ['1819921', '1'], ['2422988', '1'], ['1115649', '1'], ['921912', '1'], ['418360', '1'], ['409355', '1'], ['1183465', '1'], ['446468', '1'], ['1815652', '2'], ['987463', '1'], ['1895643', '1'], ['1239623', '1'], ['2332651', '1'], ['1961602', '2'], ['2320394', '1'], ['28312', '1'], ['1201474', '1'], ['1799962', '1'], ['1807073', '1'], ['1514559', '1'], ['2268350', '1'], ['2180065', '1'], ['1403057', '1'], ['1096756', '1'], ['325979', '1'], ['329080', '1'], ['735546', '1'], ['410567', '1'], ['1912143', '1'], ['32\n",
            "\"80cm-1ec-80ei-3400em-2200epm\", \"index_map_execution_time\", \"6.068294525146484\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('it', [['1781133', '1']]), ('was', [['1781133', '1']]), ('used', [['1781133', '2']]), ('in', [['1781133', '6']]), ('landing', [['1781133', '1']]), ('craft', [['1781133', '1']]), ('during', [['1781133', '1']]), ('world', [['1781133', '1']]), ('war', [['1781133', '1']]), ('ii', [['1781133', '1']])]\n",
            "\"80cm-1ec-80ei-3400em-2200epm\", \"index_reduce_execution_time\", \"51.22210073471069\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('useful', [['443815', '1'], ['2329563', '1'], ['73954', '1'], ['1737480', '2'], ['17745', '1'], ['547773', '1'], ['1483902', '1'], ['2288218', '1'], ['598393', '1'], ['426292', '1'], ['642432', '1'], ['1795967', '1'], ['466187', '1'], ['1659364', '1'], ['1090319', '1'], ['271502', '1'], ['1208827', '1'], ['2382634', '1'], ['183741', '1'], ['504934', '2'], ['258449', '2'], ['448639', '1'], ['1066409', '1'], ['1901244', '1'], ['1850338', '1'], ['359386', '1'], ['1936227', '1'], ['449810', '1'], ['1168611', '1'], ['439588', '1'], ['380879', '1'], ['850811', '1'], ['1856426', '1'], ['770031', '1'], ['310229', '1'], ['800212', '1'], ['220753', '1'], ['677395', '1'], ['961861', '1'], ['1912096', '1'], ['1149634', '1'], ['1438820', '1'], ['1618326', '1'], ['1272560', '1'], ['1889203', '1'], ['2162464', '1'], ['2178700', '1'], ['2271078', '1'], ['493058', '1'], ['481465', '1'], ['1731957', '1'], ['479782', '1'], ['1242738', '1'], ['272616', '1'], ['357066', '1'], ['2115517', '1'], ['602954',\n",
            "\"64cm-1ec-64ei-3400em-2200epm\", \"index_map_execution_time\", \"6.079436302185059\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('it', [['1781133', '1']]), ('was', [['1781133', '1']]), ('used', [['1781133', '2']]), ('in', [['1781133', '6']]), ('landing', [['1781133', '1']]), ('craft', [['1781133', '1']]), ('during', [['1781133', '1']]), ('world', [['1781133', '1']]), ('war', [['1781133', '1']]), ('ii', [['1781133', '1']])]\n",
            "\"64cm-1ec-64ei-3400em-2200epm\", \"index_reduce_execution_time\", \"49.724003076553345\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('herkimer', [['535783', '1'], ['57977', '1'], ['58222', '1'], ['1531614', '1'], ['181163', '2'], ['106020', '1'], ['105954', '1'], ['947790', '1'], ['106070', '1'], ['1829772', '1'], ['295833', '2'], ['701522', '1'], ['1750300', '1'], ['1379638', '1'], ['870851', '1'], ['2451714', '1'], ['105948', '1'], ['57908', '3'], ['1079398', '3'], ['58041', '1'], ['531124', '3'], ['1151424', '1'], ['57907', '1'], ['1909962', '3'], ['1282320', '1'], ['1356515', '1']]), ('meux', [['479131', '1'], ['1690605', '1'], ['1761833', '1'], ['880617', '2'], ['588575', '1'], ['1658057', '5']]), ('binging', [['2106612', '1']]), ('cleversley', [['508869', '1']]), ('newsboys', [['181251', '1'], ['1435368', '1'], ['567420', '1'], ['1819854', '1'], ['907769', '4'], ['1097107', '1'], ['453366', '1'], ['313986', '1'], ['535578', '1'], ['362501', '1'], ['502207', '1'], ['2047414', '1'], ['1783592', '1'], ['1674998', '1'], ['627357', '1'], ['661132', '1'], ['778399', '1'], ['228246', '1'], ['870109', '1']]), ('pede\n",
            "\"48cm-1ec-48ei-3400em-2200epm\", \"index_map_execution_time\", \"6.258608102798462\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('it', [['1781133', '1']]), ('was', [['1781133', '1']]), ('used', [['1781133', '2']]), ('in', [['1781133', '6']]), ('landing', [['1781133', '1']]), ('craft', [['1781133', '1']]), ('during', [['1781133', '1']]), ('world', [['1781133', '1']]), ('war', [['1781133', '1']]), ('ii', [['1781133', '1']])]\n",
            "\"48cm-1ec-48ei-3400em-2200epm\", \"index_reduce_execution_time\", \"50.21621918678284\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('timeframe', [['1671729', '1'], ['251233', '1'], ['1685665', '1'], ['293400', '1'], ['958009', '1'], ['1820991', '1'], ['227117', '1'], ['1873206', '1'], ['1950728', '1'], ['1762010', '1'], ['1862112', '1'], ['1649793', '1'], ['2175272', '1'], ['648984', '1'], ['1222363', '1'], ['847266', '1'], ['1062396', '1'], ['130408', '1'], ['699898', '1'], ['1678724', '1'], ['640348', '1'], ['1761248', '1'], ['1812228', '1'], ['940553', '1'], ['258678', '1'], ['1412645', '1'], ['2072806', '1'], ['1708081', '1'], ['1881531', '1'], ['1550622', '1'], ['1282949', '1'], ['2293548', '1'], ['10946', '1'], ['848781', '1'], ['353314', '1'], ['383876', '1'], ['1864437', '1'], ['634933', '1'], ['842947', '1'], ['96253', '1'], ['937899', '1'], ['379771', '1'], ['2417473', '1'], ['2161625', '1'], ['169154', '1'], ['795959', '1'], ['208056', '1'], ['630485', '1'], ['99202', '1'], ['898336', '1'], ['1008253', '1'], ['992251', '1'], ['1932728', '1'], ['632980', '1'], ['1719606', '1'], ['220681', '1']]), ('depa\n",
            "\"32cm-1ec-32ei-3400em-2200epm\", \"index_map_execution_time\", \"6.069242715835571\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('it', [['1781133', '1']]), ('was', [['1781133', '1']]), ('used', [['1781133', '2']]), ('in', [['1781133', '6']]), ('landing', [['1781133', '1']]), ('craft', [['1781133', '1']]), ('during', [['1781133', '1']]), ('world', [['1781133', '1']]), ('war', [['1781133', '1']]), ('ii', [['1781133', '1']])]\n",
            "\"32cm-1ec-32ei-3400em-2200epm\", \"index_reduce_execution_time\", \"51.12250781059265\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('succeeded', [['1962423', '1'], ['1922332', '1'], ['593339', '1'], ['1637068', '2'], ['1345430', '1'], ['158459', '2'], ['474981', '1'], ['156093', '1'], ['625837', '1'], ['232588', '1'], ['1291504', '1'], ['2309594', '1'], ['1485194', '1'], ['1054325', '1'], ['376471', '1'], ['1114130', '2'], ['2309833', '1'], ['1158259', '1'], ['1948418', '1'], ['1026572', '1'], ['168608', '1'], ['1748124', '1'], ['660659', '1'], ['1617095', '1'], ['1044972', '1'], ['650414', '1'], ['428274', '1'], ['210131', '2'], ['1919282', '1'], ['1007171', '1'], ['1908071', '1'], ['638168', '1'], ['2423257', '1'], ['1989707', '1'], ['293984', '1'], ['2217670', '1'], ['1485858', '1'], ['1499769', '1'], ['2004357', '1'], ['282297', '1'], ['331499', '1'], ['1696596', '1'], ['1294830', '1'], ['2236535', '1'], ['1162919', '1'], ['140099', '1'], ['908035', '2'], ['995471', '1'], ['1979671', '1'], ['2447456', '1'], ['1792913', '1'], ['83103', '1'], ['1066409', '1'], ['1691203', '1'], ['1304977', '1'], ['1073689', '1'\n",
            "\"16cm-1ec-16ei-3400em-2200epm\", \"index_map_execution_time\", \"6.279751539230347\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('it', [['1781133', '1']]), ('was', [['1781133', '1']]), ('used', [['1781133', '2']]), ('in', [['1781133', '6']]), ('landing', [['1781133', '1']]), ('craft', [['1781133', '1']]), ('during', [['1781133', '1']]), ('world', [['1781133', '1']]), ('war', [['1781133', '1']]), ('ii', [['1781133', '1']])]\n",
            "\"16cm-1ec-16ei-3400em-2200epm\", \"index_reduce_execution_time\", \"50.219305992126465\"\n",
            "train_docs_index_rdd.getNumPartitions(): 13\n",
            "elements: [('accomplish', [['1634473', '1'], ['1339636', '1'], ['371608', '1'], ['530743', '1'], ['136489', '1'], ['636821', '1'], ['1449522', '1'], ['344138', '1'], ['299177', '1'], ['530466', '1'], ['1392756', '1'], ['2231623', '1'], ['926221', '1'], ['762560', '1'], ['1445265', '1'], ['211766', '1'], ['191552', '1'], ['1210349', '1'], ['753560', '1'], ['1638047', '1'], ['1351892', '1'], ['1318865', '1'], ['893511', '1'], ['2293018', '1'], ['1104652', '1'], ['635033', '1'], ['2280817', '1'], ['1195283', '1'], ['1970684', '1'], ['122008', '1'], ['254393', '2'], ['1935675', '1'], ['1291833', '1'], ['617836', '1'], ['1194636', '1'], ['743686', '1'], ['770922', '1'], ['2162096', '1'], ['1958176', '1'], ['1217915', '1'], ['194887', '1'], ['314647', '1'], ['1883324', '1'], ['513469', '1'], ['2391233', '1'], ['1503880', '1'], ['1234722', '1'], ['133640', '1'], ['2334587', '1'], ['2415937', '1'], ['1599434', '1'], ['2101189', '1'], ['89146', '1'], ['1609478', '1'], ['276975', '1'], ['756107', '1'], ['\n",
            "program complete\n"
          ]
        }
      ],
      "source": [
        "import ir_datasets\n",
        "from pyspark import SparkContext, StorageLevel\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "import mysql.connector\n",
        "\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import math\n",
        "import base64\n",
        "\n",
        "# variables for non-experimental, trial execution (replication of code execution)\n",
        "spark_host = \"spark://10.0.0.166:7077\"  # ignored for remote execution\n",
        "max_cores = 96\n",
        "max_mem_mb = 330000\n",
        "\n",
        "# variables for conditional program flow\n",
        "# remote execution is colab execution\n",
        "# experiment execution is for execution of program with varying resource allocations (requires v-28 instance on colab, manual configuration otherwise)\n",
        "remote_execution = True\n",
        "run_multiple_configurations = True\n",
        "memory_factor = 1.0\n",
        "core_segments = 6\n",
        "run_bm25_evaluation = False\n",
        "use_large_dataset = False\n",
        "repartition_rdds = False\n",
        "reparition_multiple_of_executors = 6\n",
        "\n",
        "# create directories for datasets and outputs\n",
        "if remote_execution:\n",
        "    root_directory = \"/content/drive/MyDrive/CS532-FinalProject\"\n",
        "else:\n",
        "    root_directory = \".\"\n",
        "    mysql_connection_url = \"jdbc:mysql://localhost:3306/wikipedia_docs\"\n",
        "dataset_directory = f\"{root_directory}/data/ir_datasets\"\n",
        "log_directory = f\"{root_directory}/output\"\n",
        "if use_large_dataset:\n",
        "    training_dataset_name = \"wikir/en78k/training\"\n",
        "else:\n",
        "    training_dataset_name = \"wikir/en1k/training\"\n",
        "training_dataset_filename = training_dataset_name.replace(\"/\", \"_\")\n",
        "if not os.path.exists(dataset_directory):\n",
        "    os.makedirs(dataset_directory)\n",
        "if not os.path.exists(log_directory):\n",
        "    os.makedirs(log_directory)\n",
        "\n",
        "# load dataset if dataset has not been loaded previously\n",
        "if not (os.path.isfile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "        and os.path.isfile(f\"{dataset_directory}/{training_dataset_filename}_queries\")\n",
        "        and os.path.isfile(f\"{dataset_directory}/{training_dataset_filename}_qrels\")):\n",
        "    train_dataset = ir_datasets.load(training_dataset_name)\n",
        "\n",
        "    docs_file = open(f\"{dataset_directory}/{training_dataset_filename}_docs\", 'a')\n",
        "    csv_writer = csv.writer(docs_file, dialect='unix')\n",
        "    for doc in train_dataset.docs_iter():\n",
        "        csv_writer.writerow([doc.doc_id, doc.text])\n",
        "    docs_file.close()\n",
        "\n",
        "    queries_file = open(f\"{dataset_directory}/{training_dataset_filename}_queries\", 'a')\n",
        "    csv_writer = csv.writer(queries_file, dialect='unix')\n",
        "    for query in train_dataset.queries_iter():\n",
        "        csv_writer.writerow([query.query_id, query.text])\n",
        "    queries_file.close()\n",
        "\n",
        "    qrels_file = open(f\"{dataset_directory}/{training_dataset_filename}_qrels\", 'a')\n",
        "    csv_writer = csv.writer(qrels_file, dialect='unix')\n",
        "    for qrel in train_dataset.qrels_iter():\n",
        "        csv_writer.writerow([qrel.query_id, qrel.doc_id, qrel.relevance, qrel.iteration])\n",
        "    qrels_file.close()\n",
        "\n",
        "\n",
        "# define Spark configurations\n",
        "class SparkConfHolder:\n",
        "    def __init__(self, cores_max, executor_cores, executor_instances, executor_memory, executor_pyspark_memory):\n",
        "        self.cores_max = cores_max\n",
        "        self.executor_cores = executor_cores\n",
        "        self.executor_instances = executor_instances\n",
        "        self.executor_memory = executor_memory\n",
        "        self.executor_pyspark_memory = executor_pyspark_memory\n",
        "\n",
        "    def get_conf(self):\n",
        "        spark_conf = SparkConf()\n",
        "        spark_conf.setAll([(\"spark.cores.max\", f\"{int(self.cores_max)}\"),\n",
        "                           (\"spark.executor.cores\", \"1\"),\n",
        "                           (\"spark.executor.instances\", f\"{int(self.executor_instances)}\"),\n",
        "                           (\"spark.executor.memory\", f\"{int(self.executor_memory)}m\"),\n",
        "                           (\"spark.executor.pyspark.memory\", f\"{int(self.executor_pyspark_memory)}m\"),\n",
        "                           (\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\")])\n",
        "        return spark_conf\n",
        "\n",
        "    def get_executor_count(self):\n",
        "        return self.executor_instances\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{int(self.cores_max)}cm-{int(self.executor_cores)}ec-{int(self.executor_instances)}ei-{int(self.executor_memory)}em-{int(self.executor_pyspark_memory)}epm\"\n",
        "\n",
        "\n",
        "spark_conf_holders = list()\n",
        "executor_memory = int((math.floor(max_mem_mb/max_cores) * memory_factor // 100) * 100)\n",
        "executor_pyspark_memory = int((((2/3) * math.floor(max_mem_mb/max_cores) * memory_factor) // 100) * 100)\n",
        "if run_multiple_configurations:\n",
        "    segments = min(max_cores, core_segments)\n",
        "    core_diff_per_segment = max_cores // segments\n",
        "    spark_conf_holders.append(SparkConfHolder(max_cores, 1, max_cores, executor_memory, executor_pyspark_memory))\n",
        "    for i in range(segments - 1, 0, -1):\n",
        "        spark_conf_holders.append(SparkConfHolder(i * core_diff_per_segment, 1, i * core_diff_per_segment, executor_memory, executor_pyspark_memory))\n",
        "else:\n",
        "    spark_conf_holders.append(SparkConfHolder(max_cores, 1, max_cores, executor_memory, executor_pyspark_memory))\n",
        "\n",
        "# define methods for execution by RDDs\n",
        "\n",
        "# map corpus documents to corpus vocabulary term and term posting pairs\n",
        "# D -> (D.text.word, (D.doc_id, count(D.text.word | D.doc_id))\n",
        "def inverted_index_map_function(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[0])\n",
        "    words_counter_for_doc = Counter(re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' '))\n",
        "    words_for_doc = list(words_counter_for_doc.keys())\n",
        "    word_postings_for_doc = list([[doc_id, str(words_counter_for_doc[words_for_doc[i]])]] for i in range(0, len(words_for_doc)))\n",
        "    return list(zip(words_for_doc, word_postings_for_doc))\n",
        "\n",
        "# reduce corpus vocabulary term and corpus document id pairs to map of vocab terms to doc id lists\n",
        "# list((term, (doc_id, term_count))) -> dict({term: list((doc_id, term_count))})\n",
        "def inverted_index_reduce_function(list_of_doc_ids_for_term_instance_1, list_of_doc_ids_for_term_instance_2):\n",
        "    list_of_doc_ids_for_term_instance_1 += list_of_doc_ids_for_term_instance_2\n",
        "    return list_of_doc_ids_for_term_instance_1\n",
        "\n",
        "# translate inverted index to MySQL storage format\n",
        "def inverted_index_pickle_function(rdd_element):\n",
        "    return (rdd_element[0], \"{\\\"postings\\\": \\\"\" + base64.b64encode(pickle.dumps(rdd_element[1])).decode('utf-8') + \"\\\"}\")\n",
        "\n",
        "# return RDD with document lengths\n",
        "# used for computing the max and mean document lengths for use in the BM25 algorithm\n",
        "def compute_doc_lengths(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    words_for_doc = list(set(re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' ')))\n",
        "    return len(words_for_doc)\n",
        "\n",
        "# return RDD with mappings of doc_ids on to doc_lengths\n",
        "# used in the BM25 algorithm\n",
        "def map_doc_lengths(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[0])\n",
        "    words_for_doc = list(set(re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' ')))\n",
        "    return (doc_id, len(words_for_doc))\n",
        "\n",
        "def parse_queries(query_rdd_element):\n",
        "    query_rdd_elements = query_rdd_element.split('\\\",\\\"')\n",
        "    query_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", query_rdd_elements[0])\n",
        "    query_terms = re.sub(\"[^A-Za-z0-9 ]\", \"\", query_rdd_elements[1]).split(\" \")\n",
        "    return (query_id, query_terms)\n",
        "\n",
        "def parse_qrels(qrels_rdd_element):\n",
        "    qrels_rdd_elements = qrels_rdd_element.split('\\\",\\\"')\n",
        "    doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", qrels_rdd_elements[0])\n",
        "    query_id = doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", qrels_rdd_elements[1])\n",
        "    return (doc_id, query_id)\n",
        "\n",
        "def get_list_element_idx_0(rdd_element):\n",
        "    return rdd_element[0]\n",
        "\n",
        "def get_list_element_idx_1(rdd_element):\n",
        "    return rdd_element[1]\n",
        "\n",
        "def get_list_element_idx_2(rdd_element):\n",
        "    return rdd_element[2]\n",
        "\n",
        "def get_list_element_idx_3(rdd_element):\n",
        "    return rdd_element[3]\n",
        "\n",
        "def get_list_element_idx_4(rdd_element):\n",
        "    return rdd_element[4]\n",
        "\n",
        "def get_list_element_idx_5(rdd_element):\n",
        "    return rdd_element[5]\n",
        "\n",
        "def run_bm_25_and_qrel_eval(query_rdd_element):\n",
        "    mysql_connection = mysql.connector.connect(user='root', password='root', database='wikipedia_docs')\n",
        "    recall_for_query = dict()\n",
        "    query_id = str(query_rdd_element[0])\n",
        "    cursor = mysql_connection.cursor()\n",
        "    mysql_query = \"select term, JSON_EXTRACT(postings, \\\"$.postings\\\") as posting_enc from wikipedia_vocabulary_to_posting_lookup where term in (\"\n",
        "    for i in range(0, len(query_rdd_element[1]) - 1):\n",
        "        mysql_query = mysql_query + \"\\\"\" + query_rdd_element[1][i] + \"\\\",\"\n",
        "    mysql_query = mysql_query + \"\\\"\" + query_rdd_element[1][-1] + \"\\\");\"\n",
        "    cursor.execute(mysql_query)\n",
        "    term_and_postings_dict = dict()\n",
        "    for (term, posting_enc) in cursor:\n",
        "        term_and_postings_dict[str(term)] = dict(pickle.loads(base64.b64decode(str(posting_enc).encode('utf-8'))))\n",
        "    cursor.close()\n",
        "    doc_ids = list()\n",
        "    for postings in term_and_postings_dict.values():\n",
        "        doc_ids.extend(list(postings.keys()))\n",
        "    doc_ids = list(set(int(doc_id) for doc_id in doc_ids))\n",
        "    mysql_query = \"select doc_id, doc_length from wikipedia_doc_lengths where doc_id in (\"\n",
        "    for i in range(0, len(doc_ids)-1):\n",
        "        mysql_query = mysql_query + str(doc_ids[i]) + \", \"\n",
        "    mysql_query = mysql_query + str(doc_ids[-1]) + \");\"\n",
        "    cursor = mysql_connection.cursor()\n",
        "    cursor.execute(mysql_query)\n",
        "    doc_ids_and_lengths_list = dict()\n",
        "    for (doc_id, doc_length) in cursor:\n",
        "        doc_ids_and_lengths_list[int(doc_id)] = float(doc_length)\n",
        "    cursor.close()\n",
        "    mysql_query = \"select count(*) as count, avg(doc_length) as average from wikipedia_doc_lengths;\"\n",
        "    cursor = mysql_connection.cursor()\n",
        "    cursor.execute(mysql_query)\n",
        "    N = 1\n",
        "    doc_length_average = 1\n",
        "    for (count, average_length) in cursor:\n",
        "        N = int(count)\n",
        "        doc_length_average = float(average_length)\n",
        "    cursor.close()\n",
        "    k = 1.5\n",
        "    b = 0.75\n",
        "    doc_ids_and_rankings = dict()\n",
        "    for doc_id in doc_ids:\n",
        "        for query_term in query_rdd_element[1]:\n",
        "            term_postings = term_and_postings_dict.get(query_term)\n",
        "            term_frequency_per_doc_in_corpus = len(term_postings) if term_postings is not None else 1\n",
        "            term_frequency_in_doc = term_postings.get(doc_id) if term_postings is not None else None\n",
        "            term_frequency_in_doc = term_frequency_in_doc if term_frequency_in_doc is not None else 1\n",
        "            doc_length = doc_ids_and_lengths_list.get(doc_id)\n",
        "            doc_length = doc_length if doc_length is not None else 1\n",
        "            bm25_summand = (np.log(N / term_frequency_per_doc_in_corpus) *\n",
        "                            ((k + 1) * term_frequency_in_doc) /\n",
        "                            (k * ((1 - b) + b * (doc_length / doc_length_average)) + term_frequency_in_doc))\n",
        "            current_value = doc_ids_and_rankings.get(doc_id)\n",
        "            current_value = current_value if current_value is not None else 0\n",
        "            doc_ids_and_rankings[doc_id] = current_value + bm25_summand\n",
        "    doc_ids_sorted_by_rank = sorted(doc_ids_and_rankings.items(), key=lambda item: item[1])\n",
        "    mysql_query = \"select doc_id, query_id from wikipedia_qrels where query_id = \" + str(query_id) + \";\"\n",
        "    cursor = mysql_connection.cursor()\n",
        "    cursor.execute(mysql_query)\n",
        "    doc_ids_for_query = list()\n",
        "    for (doc_id, query_id_from_cursor) in cursor:\n",
        "        doc_ids_for_query.append(int(doc_id))\n",
        "    cursor.close()\n",
        "    recall_for_query[query_id] = [\n",
        "        0, 0, 0, 0, 0, 0\n",
        "    ]\n",
        "    for doc_id in doc_ids_for_query:\n",
        "        recall_for_query[query_id][0] = recall_for_query[query_id][0] + int(doc_id in doc_ids_sorted_by_rank[0:1])\n",
        "        recall_for_query[query_id][1] = recall_for_query[query_id][1] + int(doc_id in doc_ids_sorted_by_rank[0:5])\n",
        "        recall_for_query[query_id][2] = recall_for_query[query_id][2] + int(doc_id in doc_ids_sorted_by_rank[0:10])\n",
        "        recall_for_query[query_id][3] = recall_for_query[query_id][3] + int(doc_id in doc_ids_sorted_by_rank[0:50])\n",
        "        recall_for_query[query_id][4] = recall_for_query[query_id][4] + int(doc_id in doc_ids_sorted_by_rank[0:100])\n",
        "        recall_for_query[query_id][5] = recall_for_query[query_id][5] + int(doc_id in doc_ids_sorted_by_rank[0:1000])\n",
        "    recall_for_query[query_id][0] = recall_for_query[query_id][0] / len(doc_ids_for_query)\n",
        "    recall_for_query[query_id][1] = recall_for_query[query_id][1] / len(doc_ids_for_query)\n",
        "    recall_for_query[query_id][2] = recall_for_query[query_id][2] / len(doc_ids_for_query)\n",
        "    recall_for_query[query_id][3] = recall_for_query[query_id][3] / len(doc_ids_for_query)\n",
        "    recall_for_query[query_id][4] = recall_for_query[query_id][4] / len(doc_ids_for_query)\n",
        "    recall_for_query[query_id][5] = recall_for_query[query_id][5] / len(doc_ids_for_query)\n",
        "    mysql_connection.close()\n",
        "    return recall_for_query.items()\n",
        "\n",
        "for spark_conf_holder in spark_conf_holders:\n",
        "    log_file_name = f\"{log_directory}/{repr(spark_conf_holder)}-{int(time.time())}.log\"\n",
        "    if remote_execution:\n",
        "        storage_level = StorageLevel(True, False, False, False, 3)\n",
        "        spark_context = SparkContext(conf=spark_conf_holder.get_conf())\n",
        "    else:\n",
        "        storage_level = StorageLevel(True, True, False, False, 3)\n",
        "        spark_context = SparkContext(master=spark_host, conf=spark_conf_holder.get_conf())\n",
        "    spark_session = SparkSession(spark_context)\n",
        "    # load dataset from CSV file to RDD\n",
        "    train_docs_index_rdd = spark_context.textFile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "    if repartition_rdds:\n",
        "        train_docs_index_rdd = train_docs_index_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "    train_docs_index_rdd.persist(storage_level)\n",
        "\n",
        "    # map CSV file to vocabulary-document-id pairs, flattening pairs across documents\n",
        "    elements = train_docs_index_rdd.take(10)\n",
        "    index_map_start_time = time.time()\n",
        "    train_docs_index_rdd = train_docs_index_rdd.flatMap(inverted_index_map_function)\n",
        "    if repartition_rdds:\n",
        "        train_docs_index_rdd = train_docs_index_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "    train_docs_index_rdd.persist(storage_level)\n",
        "    elements = train_docs_index_rdd.take(10)\n",
        "    index_map_end_time = time.time()\n",
        "    output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"index_map_execution_time\\\", \\\"{index_map_end_time - index_map_start_time}\\\"\"\n",
        "    log_file = open(log_file_name, \"a\")\n",
        "    log_file.write(output_line + \"\\n\")\n",
        "    log_file.close()\n",
        "    print(output_line)\n",
        "    print(f\"train_docs_index_rdd.getNumPartitions(): {train_docs_index_rdd.getNumPartitions()}\")\n",
        "    print(f\"elements: {str(elements)[0:1000]}\")\n",
        "\n",
        "    elements = train_docs_index_rdd.take(10)\n",
        "    index_reduce_start_time = time.time()\n",
        "    train_docs_index_rdd = train_docs_index_rdd.reduceByKey(inverted_index_reduce_function)\n",
        "    if repartition_rdds:\n",
        "        train_docs_index_rdd = train_docs_index_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "    train_docs_index_rdd.persist(storage_level)\n",
        "    elements = train_docs_index_rdd.take(10)\n",
        "    index_reduce_end_time = time.time()\n",
        "    output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"index_reduce_execution_time\\\", \\\"{index_reduce_end_time - index_reduce_start_time}\\\"\"\n",
        "    log_file = open(log_file_name, \"a\")\n",
        "    log_file.write(output_line + \"\\n\")\n",
        "    log_file.close()\n",
        "    print(output_line)\n",
        "    print(f\"train_docs_index_rdd.getNumPartitions(): {train_docs_index_rdd.getNumPartitions()}\")\n",
        "    print(f\"elements: {str(elements)[0:1000]}\")\n",
        "\n",
        "    if run_bm25_evaluation:\n",
        "        elements = train_docs_index_rdd.take(10)\n",
        "        index_pickle_start_time = time.time()\n",
        "        train_docs_index_rdd = train_docs_index_rdd.map(inverted_index_pickle_function)\n",
        "        if repartition_rdds:\n",
        "            train_docs_index_rdd = train_docs_index_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        train_docs_index_rdd.persist(storage_level)\n",
        "        elements = train_docs_index_rdd.take(10)\n",
        "        index_pickle_end_time = time.time()\n",
        "        output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"index_pickle_execution_time\\\", \\\"{index_pickle_end_time - index_pickle_start_time}\\\"\"\n",
        "        log_file = open(log_file_name, \"a\")\n",
        "        log_file.write(output_line + \"\\n\")\n",
        "        log_file.close()\n",
        "        print(output_line)\n",
        "        print(f\"train_docs_index_rdd.getNumPartitions(): {train_docs_index_rdd.getNumPartitions()}\")\n",
        "        print(f\"elements: {str(elements)[0:1000]}\")\n",
        "\n",
        "        index_store_start_time = time.time()\n",
        "        train_docs_index_df = spark_session.createDataFrame(train_docs_index_rdd, schema=[\"term\", \"postings\"])\n",
        "        train_docs_index_df.write.jdbc(\n",
        "            url=mysql_connection_url,\n",
        "            table=\"wikipedia_vocabulary_to_posting_lookup\",\n",
        "            properties={\n",
        "                \"user\": \"root\",\n",
        "                \"password\": \"root\",\n",
        "                \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "            },\n",
        "            mode=\"overwrite\"\n",
        "        )\n",
        "        index_store_end_time = time.time()\n",
        "        output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"index_store_execution_time\\\", \\\"{index_store_end_time - index_store_start_time}\\\"\"\n",
        "        log_file = open(log_file_name, \"a\")\n",
        "        log_file.write(output_line + \"\\n\")\n",
        "        log_file.close()\n",
        "        print(output_line)\n",
        "\n",
        "        train_docs_len_map_rdd = spark_context.textFile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "        if repartition_rdds:\n",
        "            train_docs_len_map_rdd = train_docs_len_map_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        train_docs_len_map_rdd.persist(storage_level)\n",
        "        elements = train_docs_len_map_rdd.take(10)\n",
        "        doc_len_map_start_time = time.time()\n",
        "        train_docs_len_map_rdd = train_docs_len_map_rdd.map(map_doc_lengths)\n",
        "        if repartition_rdds:\n",
        "            train_docs_len_map_rdd = train_docs_len_map_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        train_docs_len_map_rdd.persist(storage_level)\n",
        "        elements = train_docs_len_map_rdd.take(10)\n",
        "        doc_len_map_end_time = time.time()\n",
        "        output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"doc_length_map_execution_time\\\", \\\"{doc_len_map_end_time - doc_len_map_start_time}\\\"\"\n",
        "        log_file = open(log_file_name, \"a\")\n",
        "        log_file.write(output_line + \"\\n\")\n",
        "        log_file.close()\n",
        "        print(output_line)\n",
        "        print(f\"train_docs_len_map_rdd.getNumPartitions(): {train_docs_len_map_rdd.getNumPartitions()}\")\n",
        "        print(f\"elements: {str(elements)[0:1000]}\")\n",
        "\n",
        "        doc_len_store_start_time = time.time()\n",
        "        train_docs_len_map_rdd = spark_session.createDataFrame(train_docs_len_map_rdd, schema=[\"doc_id\", \"doc_length\"])\n",
        "        train_docs_len_map_rdd.write.jdbc(\n",
        "            url=mysql_connection_url,\n",
        "            table=\"wikipedia_doc_lengths\",\n",
        "            properties={\n",
        "                \"user\": \"root\",\n",
        "                \"password\": \"root\",\n",
        "                \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "            },\n",
        "            mode=\"overwrite\"\n",
        "        )\n",
        "        doc_len_store_end_time = time.time()\n",
        "        output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"doc_length_store_execution_time\\\", \\\"{doc_len_store_end_time - doc_len_store_start_time}\\\"\"\n",
        "        log_file = open(log_file_name, \"a\")\n",
        "        log_file.write(output_line + \"\\n\")\n",
        "        log_file.close()\n",
        "        print(output_line)\n",
        "\n",
        "        train_qrels_rdd = spark_context.textFile(f\"{dataset_directory}/{training_dataset_filename}_qrels\")\n",
        "        if repartition_rdds:\n",
        "            train_qrels_rdd = train_qrels_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        train_qrels_rdd.persist(storage_level)\n",
        "        elements = train_qrels_rdd.take(10)\n",
        "        qrels_parse_start_time = time.time()\n",
        "        train_qrels_rdd = train_qrels_rdd.map(parse_qrels)\n",
        "        if repartition_rdds:\n",
        "            train_qrels_rdd = train_qrels_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        train_qrels_rdd.persist(storage_level)\n",
        "        elements = train_qrels_rdd.take(10)\n",
        "        qrels_parse_end_time = time.time()\n",
        "        output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"qrels_parse_execution_time\\\", \\\"{qrels_parse_end_time - qrels_parse_start_time}\\\"\"\n",
        "        log_file = open(log_file_name, \"a\")\n",
        "        log_file.write(output_line + \"\\n\")\n",
        "        log_file.close()\n",
        "        print(output_line)\n",
        "        print(f\"train_qrels_rdd.getNumPartitions(): {train_qrels_rdd.getNumPartitions()}\")\n",
        "        print(f\"elements: {str(elements)[0:1000]}\")\n",
        "\n",
        "        qrels_store_start_time = time.time()\n",
        "        train_qrels_df = spark_session.createDataFrame(train_qrels_rdd, schema=[\"doc_id\", \"query_id\"])\n",
        "        train_qrels_df.write.jdbc(\n",
        "            url=mysql_connection_url,\n",
        "            table=\"wikipedia_qrels\",\n",
        "            properties={\n",
        "                \"user\": \"root\",\n",
        "                \"password\": \"root\",\n",
        "                \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "            },\n",
        "            mode=\"overwrite\"\n",
        "        )\n",
        "        qrels_store_end_time = time.time()\n",
        "        output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"qrels_store_execution_time\\\", \\\"{qrels_store_end_time - qrels_store_start_time}\\\"\"\n",
        "        log_file = open(log_file_name, \"a\")\n",
        "        log_file.write(output_line + \"\\n\")\n",
        "        log_file.close()\n",
        "        print(output_line)\n",
        "\n",
        "        queries_rdd = spark_context.textFile(f\"{dataset_directory}/{training_dataset_filename}_queries\")\n",
        "        if repartition_rdds:\n",
        "            queries_rdd = queries_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        queries_rdd.persist(storage_level)\n",
        "        queries_rdd = queries_rdd.map(parse_queries)\n",
        "        if repartition_rdds:\n",
        "            queries_rdd = queries_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        queries_rdd.persist(storage_level)\n",
        "        elements = queries_rdd.take(10)\n",
        "        bm25_start_time = time.time()\n",
        "        query_recall_rdd = queries_rdd.map(run_bm_25_and_qrel_eval)\n",
        "        if repartition_rdds:\n",
        "            query_recall_rdd = query_recall_rdd.repartition(spark_conf_holder.get_executor_count() * reparition_multiple_of_executors)\n",
        "        query_recall_rdd.persist(storage_level)\n",
        "        elements = query_recall_rdd.take(10)\n",
        "        bm25_end_time = time.time()\n",
        "        output_line = f\"\\\"{repr(spark_conf_holder)}\\\", \\\"bm25_and_qrel_eval_execution_time\\\", \\\"{bm25_end_time - bm25_start_time}\\\"\"\n",
        "        log_file = open(log_file_name, \"a\")\n",
        "        log_file.write(output_line + \"\\n\")\n",
        "        log_file.close()\n",
        "        print(output_line)\n",
        "        print(f\"queries_rdd.getNumPartitions(): {queries_rdd.getNumPartitions()}\")\n",
        "        print(f\"elements: {str(elements)[0:1000]}\")\n",
        "\n",
        "        query_recall_rdd_0 = query_recall_rdd.map(get_list_element_idx_0)\n",
        "        query_recall_rdd_1 = query_recall_rdd.map(get_list_element_idx_1)\n",
        "        query_recall_rdd_2 = query_recall_rdd.map(get_list_element_idx_2)\n",
        "        query_recall_rdd_3 = query_recall_rdd.map(get_list_element_idx_3)\n",
        "        query_recall_rdd_4 = query_recall_rdd.map(get_list_element_idx_4)\n",
        "        query_recall_rdd_5 = query_recall_rdd.map(get_list_element_idx_5)\n",
        "\n",
        "        recall_at_1 = query_recall_rdd_0.mean()\n",
        "        recall_at_5 = query_recall_rdd_1.mean()\n",
        "        recall_at_10 = query_recall_rdd_2.mean()\n",
        "        recall_at_50 = query_recall_rdd_3.mean()\n",
        "        recall_at_100 = query_recall_rdd_4.mean()\n",
        "        recall_at_1000 = query_recall_rdd_5.mean()\n",
        "\n",
        "        recall_output = (f\"\\\"{repr(spark_conf_holder)}\\\", \\\"recall_at_1: {recall_at_1}\\\", \\\"recall_at_5: {recall_at_5}\\\", \"\n",
        "                         + f\"\\\"recall_at_10: {recall_at_10}\\\", \\\"recall_at_50: {recall_at_50}\\\", \"\n",
        "                         + f\"\\\"recall_at_100: {recall_at_100}\\\", \\\"recall_at_1000: {recall_at_1000}\\\"\")\n",
        "        log_file.write(recall_output)\n",
        "        print(recall_output)\n",
        "\n",
        "    spark_context.stop()\n",
        "\n",
        "\n",
        "print(\"program complete\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}