{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkKBJbLHf8Te",
        "outputId": "54432175-e2e2-4e91-9005-71175be96e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ir-datasets\n",
            "  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.13.4)\n",
            "Collecting inscriptis>=2.2.0 (from ir-datasets)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting lxml>=4.5.2 (from ir-datasets)\n",
            "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.67.1)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir-datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir-datasets)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir-datasets)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir-datasets)\n",
            "  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting ijson>=3.1.3 (from ir-datasets)\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir-datasets)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (20.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2025.4.26)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18996 sha256=43e2ea4dfea97f124bbd7f140bf2ba88690b89c2349762ca1f2e4279d98ba42e\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
            "  Building wheel for cbor (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53973 sha256=f4cd38408fb0bc2c68dfa928d5a3a24399459f85099f1594fb2bb88f71a5e14a\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, lz4, lxml, inscriptis, ir-datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.3.0 inscriptis-2.6.0 ir-datasets-0.5.10 lxml-5.4.0 lz4-4.4.4 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.9\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.5-py2.py3-none-any.whl size=317747923 sha256=c154560df83596196425959f39d532011569d3e836dc3d33359326434ba0ccbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/7f/b4/0e68c6d8d89d2e582e5498ad88616c16d7c19028680e9d3840\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.5\n",
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.3.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install ir-datasets\n",
        "!pip install pyspark\n",
        "!pip install mysql-connector-python\n",
        "\n",
        "!apt-get install -y mysql-server\n",
        "!/etc/init.d/mysql start\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBOo7yQ3lxxu",
        "outputId": "d4886c59-55d1-46e1-9eff-6105cda0e7c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index map execution time: 0.44065213203430176\n",
            "[('these', [['0', '8']]), ('institutions', [['0', '5']]), ('are', [['0', '22']]), ('often', [['0', '8']]), ('described', [['0', '2']]), ('as', [['0', '92']]), ('stateless', [['0', '2']]), ('societies', [['0', '2']]), ('although', [['0', '4']]), ('several', [['0', '3']])]\n",
            "reduce execution time: 319.04465913772583\n",
            "[('civilization', [['0', '1'], ['3', '1'], ['16', '4'], ['18', '1'], ['24', '2'], ['26', '1'], ['34', '2'], ['45', '1'], ['47', '1'], ['64', '1'], ['68', '2'], ['81', '3'], ['87', '1'], ['96', '3'], ['109', '3'], ['117', '9'], ['128', '1'], ['132', '1'], ['133', '7'], ['165', '3'], ['187', '1'], ['201', '8'], ['207', '3'], ['215', '1'], ['235', '1'], ['253', '1'], ['255', '1'], ['265', '4'], ['274', '4'], ['290', '1'], ['323', '2'], ['402', '1'], ['415', '1'], ['485', '1'], ['566', '1'], ['599', '4'], ['605', '1'], ['636', '1'], ['638', '1'], ['649', '1'], ['662', '1'], ['665', '2'], ['669', '1'], ['695', '2'], ['713', '4'], ['728', '28'], ['776', '1'], ['795', '2'], ['878', '1'], ['895', '10'], ['979', '1'], ['996', '3'], ['1010', '3'], ['1072', '1'], ['1076', '2'], ['1109', '1'], ['1127', '1'], ['1163', '1'], ['1168', '1'], ['1196', '2'], ['1219', '1'], ['1223', '1'], ['1225', '1'], ['1229', '1'], ['1250', '1'], ['1254', '4'], ['1257', '1'], ['1262', '1'], ['1427', '2'], ['1437', '1'\n",
            "doc length mean execution time: 21.73195719718933\n",
            "doc length mean: 307.8184282008289 doc length max: 7205\n",
            "doc length lookup execution time: 0.2837224006652832\n",
            "[('0', 1854), ('1', 1834), ('2', 885), ('3', 393), ('4', 2978), ('5', 1734), ('6', 3128), ('7', 2344), ('8', 687), ('9', 1758)]\n",
            "program complete\n"
          ]
        }
      ],
      "source": [
        "import ir_datasets\n",
        "from pyspark import SparkContext, StorageLevel\n",
        "from pyspark import SparkConf\n",
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "root_directory = \"/content/drive/MyDrive/CS532-FinalProject\"\n",
        "# root_directory = \"/Users/mjohnson/Workspace/umass/umass-cs532/FinalProject\"\n",
        "dataset_directory = f\"{root_directory}/data/ir_datasets\"\n",
        "checkpoint_directory = f\"{root_directory}/rdd_checkpoints\"\n",
        "# training_dataset_name = \"wikir/en1k/training\"\n",
        "training_dataset_name = \"wikir/en78k/training\" # update to whole set\n",
        "training_dataset_filename = training_dataset_name.replace(\"/\", \"_\")\n",
        "\n",
        "# read dataset and write to CSV file on Google Drive\n",
        "# resolves memory issues that occur consequent to initializaing RDD from dataset\n",
        "#     iterables using SparkContext.parallelize()\n",
        "\n",
        "if not os.path.exists(dataset_directory):\n",
        "    os.makedirs(dataset_directory)\n",
        "\n",
        "# if not os.path.exists(checkpoint_directory):\n",
        "#     os.makedirs(checkpoint_directory)\n",
        "\n",
        "if not (os.path.isfile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "        and os.path.isfile(f\"{dataset_directory}/{training_dataset_filename}_queries\")\n",
        "        and os.path.isfile(f\"{dataset_directory}/{training_dataset_filename}_qrels\")):\n",
        "    train_dataset = ir_datasets.load(training_dataset_name)\n",
        "\n",
        "    docs_file = open(f\"{dataset_directory}/{training_dataset_filename}_docs\", 'w+')\n",
        "    csv_writer = csv.writer(docs_file, dialect='unix')\n",
        "    for doc in train_dataset.docs_iter():\n",
        "        csv_writer.writerow([doc.doc_id, doc.text])\n",
        "    docs_file.close()\n",
        "\n",
        "    queries_file = open(f\"{dataset_directory}/{training_dataset_filename}_queries\", 'w+')\n",
        "    csv_writer = csv.writer(queries_file, dialect='unix')\n",
        "    for query in train_dataset.queries_iter():\n",
        "        csv_writer.writerow([query.query_id, query.text])\n",
        "    queries_file.close()\n",
        "\n",
        "    qrels_file = open(f\"{dataset_directory}/{training_dataset_filename}_qrels\", 'w+')\n",
        "    csv_writer = csv.writer(qrels_file, dialect='unix')\n",
        "    for qrel in train_dataset.qrels_iter():\n",
        "        csv_writer.writerow([qrel.query_id, qrel.doc_id, qrel.relevance, qrel.iteration])\n",
        "    qrels_file.close()\n",
        "\n",
        "# map corpus documents to corpus vocabulary and document id pairs\n",
        "# D -> (D.text.word, D.doc_id)\n",
        "\n",
        "# stop standalone cluster if it exists (facilitates re-execution)\n",
        "# try:\n",
        "#     spark.stop()\n",
        "# except BaseException:\n",
        "#     pass # no op\n",
        "\n",
        "# start spark standalone cluster and load spark session context\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.cores.max\", \"96\")\n",
        "conf.set(\"spark.executor.cores\", \"1\")\n",
        "conf.set(\"spark.executor.instances\", \"96\")\n",
        "conf.set(\"spark.executor.memory\", \"3g\")\n",
        "conf.set(\"spark.executor.pyspark.memory\", \"2g\")\n",
        "spark = SparkContext(conf=conf)\n",
        "# spark = SparkContext(master=\"spark://10.0.0.166:7077\", conf=conf)\n",
        "# spark.setCheckpointDir(checkpoint_directory)\n",
        "# storage_level = StorageLevel(True, True, False, False, 3)\n",
        "storage_level = StorageLevel(True, False, False, False, 3)\n",
        "\n",
        "# load dataset from CSV file to RDD\n",
        "train_docs_index_rdd = spark.textFile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "train_docs_index_rdd.persist(storage_level)\n",
        "\n",
        "# define function for mapping CSV file lines to vocabulary-document-id pairs\n",
        "def inverted_index_map_function(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[0])\n",
        "    words_counter_for_doc = Counter(re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' '))\n",
        "    words_for_doc = list(words_counter_for_doc.keys())\n",
        "    word_postings_for_doc = list([[doc_id, str(words_counter_for_doc[words_for_doc[i]])]] for i in range(0, len(words_for_doc)))\n",
        "    return list(zip(words_for_doc, word_postings_for_doc))\n",
        "\n",
        "# map CSV file to vocabulary-document-id pairs, flattening pairs across documents\n",
        "elements = train_docs_index_rdd.take(10)\n",
        "index_map_start_time = time.time()\n",
        "train_docs_index_rdd = train_docs_index_rdd.flatMap(inverted_index_map_function)\n",
        "elements = train_docs_index_rdd.take(10)\n",
        "index_map_end_time = time.time()\n",
        "\n",
        "print(f\"index map execution time: {index_map_end_time - index_map_start_time}\")\n",
        "print(f\"{str(elements)[0:1000]}\")\n",
        "\n",
        "train_docs_index_rdd.persist(storage_level)\n",
        "\n",
        "# confirm mapping is as expected\n",
        "# elements = train_docs_rdd.take(10)\n",
        "# print(f\"elements: {str(elements)[:1000]}\")\n",
        "\n",
        "# reduce corpus vocabulary term and corpus document id pairs to map of vocab terms to doc id lists\n",
        "# list((term, (doc_id, term_count))) -> dict({term: list((doc_id, term_count))})\n",
        "\n",
        "def inverted_index_reduce_function(list_of_doc_ids_for_term_instance_1, list_of_doc_ids_for_term_instance_2):\n",
        "    list_of_doc_ids_for_term_instance_1 += list_of_doc_ids_for_term_instance_2\n",
        "    return list_of_doc_ids_for_term_instance_1\n",
        "\n",
        "elements = train_docs_index_rdd.take(10)\n",
        "index_reduce_start_time = time.time()\n",
        "train_docs_index_rdd = train_docs_index_rdd.reduceByKey(inverted_index_reduce_function)\n",
        "elements = train_docs_index_rdd.take(10)\n",
        "index_reduce_end_time = time.time()\n",
        "\n",
        "print(f\"reduce execution time: {index_reduce_end_time - index_reduce_start_time}\")\n",
        "print(f\"{str(elements)[0:1000]}\")\n",
        "\n",
        "# train_docs_index_rdd = train_docs_index_rdd.repartition(96 * 5)\n",
        "\n",
        "# elements = train_docs_index_rdd.take(1)\n",
        "# index_sort_start_time = time.time()\n",
        "# train_docs_index_rdd = train_docs_index_rdd.sortByKey()\n",
        "# elements = train_docs_index_rdd.take(1)\n",
        "# index_sort_end_time = time.time()\n",
        "\n",
        "# print(f\"sort execution time: {index_sort_end_time - index_sort_start_time}\")\n",
        "# print(f\"{str(elements)[0:1000]}\")\n",
        "\n",
        "train_docs_rdd = spark.textFile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "train_docs_rdd.persist(storage_level)\n",
        "\n",
        "N = train_docs_rdd.count()\n",
        "k = 1.5\n",
        "b = 0.75\n",
        "\n",
        "def compute_doc_lengths(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    words_for_doc = list(set(re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' ')))\n",
        "    return len(words_for_doc)\n",
        "\n",
        "elements = train_docs_rdd.take(1)\n",
        "doc_len_mean_start_time = time.time()\n",
        "train_docs_rdd = train_docs_rdd.map(compute_doc_lengths)\n",
        "doc_len_mean = train_docs_rdd.mean()\n",
        "doc_len_max = train_docs_rdd.max()\n",
        "elements = train_docs_rdd.take(1)\n",
        "doc_len_mean_end_time = time.time()\n",
        "del train_docs_rdd\n",
        "\n",
        "print(f\"doc length mean execution time: {doc_len_mean_end_time - doc_len_mean_start_time}\")\n",
        "print(f\"doc length mean: {doc_len_mean} doc length max: {doc_len_max}\")\n",
        "\n",
        "train_docs_len_lookup_rdd = spark.textFile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "train_docs_len_lookup_rdd.persist()\n",
        "\n",
        "def map_doc_lengths(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[0])\n",
        "    words_for_doc = list(set(re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' ')))\n",
        "    return (doc_id, len(words_for_doc))\n",
        "\n",
        "elements = train_docs_len_lookup_rdd.take(10)\n",
        "doc_len_lookup_start_time = time.time()\n",
        "train_docs_len_lookup_rdd = train_docs_len_lookup_rdd.map(map_doc_lengths)\n",
        "elements = train_docs_len_lookup_rdd.take(10)\n",
        "doc_len_lookup_end_time = time.time()\n",
        "\n",
        "print(f\"doc length lookup execution time: {doc_len_lookup_end_time - doc_len_lookup_start_time}\")\n",
        "print(f\"{str(elements)[0:1000]}\")\n",
        "\n",
        "# def compute_bm_25_score(rdd_element):\n",
        "#     return train_docs_len_lookup_rdd.lookup(rdd_element[0])\n",
        "\n",
        "# elements = train_docs_len_lookup_rdd.take(10)\n",
        "# bm25_lookup_start_time = time.time()\n",
        "# doc_lengths = train_docs_len_lookup_rdd.map(compute_bm_25_score)\n",
        "# elements = doc_lengths.take(10)\n",
        "# bm25_lookup_end_time = time.time()\n",
        "\n",
        "# print(f\"doc length lookup execution time: {bm25_lookup_end_time - bm25_lookup_start_time}\")\n",
        "# print(f\"{str(elements)[0:1000]}\")\n",
        "\n",
        "# train_docs_rdd.saveAsTextFile(f\"{dataset_directory}/{training_dataset_filename}_index\", compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")\n",
        "\n",
        "print(\"program complete\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}