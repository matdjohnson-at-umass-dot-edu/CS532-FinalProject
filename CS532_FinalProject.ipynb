{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matdjohnson-at-umass-dot-edu/CS532-FinalProject/blob/main/CS532_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JCU0gwK8UrKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30ebe96-bef7-4663-d10d-8515ef424761",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ir-datasets\n",
            "  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.13.3)\n",
            "Collecting inscriptis>=2.2.0 (from ir-datasets)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting lxml>=4.5.2 (from ir-datasets)\n",
            "  Downloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.67.1)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir-datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir-datasets)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir-datasets)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir-datasets)\n",
            "  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting ijson>=3.1.3 (from ir-datasets)\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir-datasets)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (19.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2025.1.31)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18995 sha256=16da400171b0eb9004c7312e7f83d2c714eb7404185755513b6f875ef4930e6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
            "  Building wheel for cbor (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53974 sha256=e4268bd070efa15dd3082d518b9a132e3d60364a1d514427a98ae226af899e4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, lz4, lxml, inscriptis, ir-datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.3.0 inscriptis-2.6.0 ir-datasets-0.5.10 lxml-5.3.2 lz4-4.4.4 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install ir-datasets\n",
        "\n",
        "import ir_datasets\n",
        "import xml.etree.ElementTree as ElementTree\n",
        "from datetime import datetime\n",
        "import re\n",
        "import textwrap\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5iYKAM_lBGr",
        "outputId": "c8236ba0-f66c-43c6-9760-9db5f2bb7e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMPqmrQSz4m1"
      },
      "outputs": [],
      "source": [
        "# ! export data_directory='/content/drive/MyDrive/CS532-FinalProject/WikipediaDump20250201-Unzipped'; \\\n",
        "#   for file in $(ls ${data_directory}); \\\n",
        "#     do echo \"$(date +%Y-%m-%d-%T) - Unzipping file ${file} ...\"; \\\n",
        "#     bzip2 -d ${data_directory}/${file}; \\\n",
        "#   done\n",
        "\n",
        "\n",
        "# wikipedia_file_directory = \"/content/drive/MyDrive/CS532-FinalProject/WikipediaDump20250201-Unzipped\"\n",
        "# sample_filename = \"enwiki-20250201-pages-meta-current1.xml-p1p41242\"\n",
        "# wikipedia_sample_file = open(f\"{wikipedia_file_directory}/{sample_filename}\")\n",
        "# print(f\"{str(datetime.now())} - start parse of XML\")\n",
        "# wikipedia_xml_tree = ElementTree.parse(wikipedia_sample_file)\n",
        "# print(f\"{str(datetime.now())} - end parse of XML\")\n",
        "# print(f\"{str(datetime.now())} - start read of XML root\")\n",
        "# wikipedia_xml_tree.findall(\".\")\n",
        "# print(f\"{str(datetime.now())} - end read of XML root\")\n",
        "\n",
        "\n",
        "# wikipedia_xml_root_children = wikipedia_xml_tree.findall('*')\n",
        "# print(f\"{len(wikipedia_xml_root_children)}\")\n",
        "# for i in range(0, 100):\n",
        "#     print(f\"element-{i}: {wikipedia_xml_root_children[i]}\")\n",
        "\n",
        "\n",
        "# namespace_prefix = '\\{.*\\}'\n",
        "# map_of_tags_to_indices = dict()\n",
        "#\n",
        "# for i in range(0, len(wikipedia_xml_root_children)):\n",
        "#     tag_for_element = re.sub(namespace_prefix, \"\", wikipedia_xml_root_children[i].tag)\n",
        "#     list_for_tag = None\n",
        "#     if map_of_tags_to_indices.get(tag_for_element) is None:\n",
        "#         list_for_tag = list()\n",
        "#         map_of_tags_to_indices[tag_for_element] = list_for_tag\n",
        "#     else:\n",
        "#         list_for_tag = map_of_tags_to_indices.get(tag_for_element)\n",
        "#     list_for_tag.append(i)\n",
        "#\n",
        "# for k, v in map_of_tags_to_indices.items():\n",
        "#     print(f\"key={k} len(value)={len(v)}\")\n",
        "#\n",
        "# print()\n",
        "# ElementTree.indent(wikipedia_xml_root_children[0])\n",
        "# print(ElementTree.tostring(wikipedia_xml_root_children[0]).decode())\n",
        "# print()\n",
        "# ElementTree.indent(wikipedia_xml_root_children[1])\n",
        "# print(ElementTree.tostring(wikipedia_xml_root_children[1]).decode())\n",
        "\n",
        "\n",
        "# namespace_prefix = '\\{.*\\}'\n",
        "# map_of_tags_to_indices = dict()\n",
        "#\n",
        "# for i in range(0, len(wikipedia_xml_root_children)):\n",
        "#     tag_for_element = re.sub(namespace_prefix, \"\", wikipedia_xml_root_children[i].tag)\n",
        "#     list_for_tag = None\n",
        "#     if map_of_tags_to_indices.get(tag_for_element) is None:\n",
        "#         list_for_tag = list()\n",
        "#         map_of_tags_to_indices[tag_for_element] = list_for_tag\n",
        "#     else:\n",
        "#         list_for_tag = map_of_tags_to_indices.get(tag_for_element)\n",
        "#     list_for_tag.append(i)\n",
        "#\n",
        "# for k, v in map_of_tags_to_indices.items():\n",
        "#     print(f\"key={k} len(value)={len(v)}\")\n",
        "#\n",
        "# print()\n",
        "# ElementTree.indent(wikipedia_xml_root_children[0])\n",
        "# print(ElementTree.tostring(wikipedia_xml_root_children[0]).decode())\n",
        "# print()\n",
        "# ElementTree.indent(wikipedia_xml_root_children[1])\n",
        "# print(ElementTree.tostring(wikipedia_xml_root_children[1]).decode())\n",
        "\n",
        "\n",
        "# tag_types = set(map_of_tags_to_indices.keys())\n",
        "# print(tag_types)\n",
        "# for element in wikipedia_xml_tree.iter():\n",
        "#     tag_for_element = re.sub(namespace_prefix, \"\", element.tag)\n",
        "#     if tag_for_element == 'mediawiki':\n",
        "#         print()\n",
        "#         ElementTree.indent(element)\n",
        "#         print(ElementTree.tostring(element).decode())\n",
        "#     # tag_for_element = element.tag\n",
        "#     tag_types.add(tag_for_element)\n",
        "# print(f\"len(tag_types)={len(tag_types)}\")\n",
        "\n",
        "\n",
        "# tag_type_print_lines = textwrap.wrap(str(tag_types))\n",
        "# for line in tag_type_print_lines:\n",
        "#     print(line)\n",
        "\n",
        "\n",
        "# for element in wikipedia_xml_root_children:\n",
        "#     tag_for_element = re.sub(namespace_prefix, \"\", element.tag)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = \"/content/drive/MyDrive/CS532-FinalProject/ir_datasets\"\n",
        "dataset_name = \"wikir/en1k/training\"\n",
        "dataset_filename = dataset_name.replace(\"/\", \"_\")\n",
        "\n",
        "train_dataset = ir_datasets.load(\"wikir/en1k/training\")\n",
        "\n",
        "docs_file = open(f\"{dataset_directory}/{dataset_filename}\", 'w+')\n",
        "csv_writer = csv.writer(docs_file, dialect='unix')\n",
        "\n",
        "for doc in train_dataset.docs_iter():\n",
        "    csv_writer.writerow([doc.doc_id, doc.text])\n",
        "\n",
        "docs_file.close()"
      ],
      "metadata": {
        "id": "9yZJPcCxJmEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, StorageLevel\n",
        "\n",
        "if spark is not None:\n",
        "    spark.stop()\n",
        "spark = SparkContext()\n",
        "\n",
        "train_rdd = spark.textFile(f\"{dataset_directory}/{dataset_filename}\")\n",
        "train_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "def map_function(row):\n",
        "    csv_file_line_elements = row.split('\\\",\\\"')\n",
        "    doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[0])\n",
        "    words_for_row = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' ')\n",
        "    return list(zip(words_for_row, list(doc_id for i in range(0, len(words_for_row)))))\n",
        "\n",
        "train_rdd = train_rdd.flatMap(map_function)\n"
      ],
      "metadata": {
        "id": "95wlfHH-CAWF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}