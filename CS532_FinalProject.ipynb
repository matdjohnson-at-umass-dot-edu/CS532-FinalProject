{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matdjohnson-at-umass-dot-edu/CS532-FinalProject/blob/main/CS532_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JCU0gwK8UrKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f7a573-4417-4ebc-c52e-568ea9c3d40c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.11/dist-packages (0.5.10)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.13.3)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.6.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.67.1)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (4.4.4)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (0.2.3)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets) (19.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets) (2025.1.31)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install ir-datasets\n",
        "!pip install pyspark\n",
        "\n",
        "import ir_datasets\n",
        "from pyspark import SparkContext, StorageLevel\n",
        "\n",
        "import re\n",
        "import csv\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5iYKAM_lBGr",
        "outputId": "39ab4858-ec8e-4b0d-bcf2-c4ebe1a45067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_directory = \"/content/drive/MyDrive/CS532-FinalProject/ir_datasets\"\n",
        "training_dataset_name = \"wikir/en1k/training\"\n",
        "training_dataset_filename = training_dataset_name.replace(\"/\", \"_\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset and write to CSV file on Google Drive\n",
        "# resolves memory issues that occur consequent to initializaing RDD from dataset\n",
        "#     iterables using SparkContext.parallelize()\n",
        "\n",
        "train_dataset = ir_datasets.load(training_dataset_name)\n",
        "\n",
        "docs_file = open(f\"{dataset_directory}/{training_dataset_filename}_docs\", 'w+')\n",
        "csv_writer = csv.writer(docs_file, dialect='unix')\n",
        "for doc in train_dataset.docs_iter():\n",
        "    csv_writer.writerow([doc.doc_id, doc.text])\n",
        "docs_file.close()\n",
        "\n",
        "queries_file = open(f\"{dataset_directory}/{training_dataset_filename}_queries\", 'w+')\n",
        "csv_writer = csv.writer(queries_file, dialect='unix')\n",
        "for query in train_dataset.queries_iter():\n",
        "    csv_writer.writerow([query.query_id, query.text])\n",
        "queries_file.close()\n",
        "\n",
        "qrels_file = open(f\"{dataset_directory}/{training_dataset_filename}_qrels\", 'w+')\n",
        "csv_writer = csv.writer(qrels_file, dialect='unix')\n",
        "for qrel in train_dataset.qrels_iter():\n",
        "    csv_writer.writerow([qrel.query_id, qrel.doc_id, qrel.relevance, qrel.iteration])\n",
        "qrels_file.close()"
      ],
      "metadata": {
        "id": "9yZJPcCxJmEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a863c900-9302-4aee-874f-30087d02db4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] If you have a local copy of https://zenodo.org/record/3565761/files/wikIR1k.zip, you can symlink it here to avoid downloading it again: /root/.ir_datasets/downloads/554299bca984640cb283d6ba55753608\n",
            "[INFO] [starting] https://zenodo.org/record/3565761/files/wikIR1k.zip\n",
            "[INFO] [finished] https://zenodo.org/record/3565761/files/wikIR1k.zip: [00:06] [165MB] [24.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map corpus documents to corpus vocabulary and document id pairs\n",
        "# D -> (D.text.word, D.doc_id)\n",
        "\n",
        "# stop standalone cluster if it exists (facilitates re-execution)\n",
        "try:\n",
        "    spark.stop()\n",
        "except NameError:\n",
        "    pass # no op\n",
        "\n",
        "# start spark standalone cluster and load spark session context\n",
        "spark = SparkContext()\n",
        "\n",
        "# load dataset from CSV file to RDD\n",
        "train_rdd = spark.textFile(f\"{dataset_directory}/{training_dataset_filename}_docs\")\n",
        "train_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# define function for mapping CSV file lines to vocabulary-document-id pairs\n",
        "def map_function(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    doc_id = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[0])\n",
        "    words_for_doc = re.sub(\"[^A-Za-z0-9 ]\", \"\", csv_file_line_elements[1]).lower().split(' ')\n",
        "    return list(zip(words_for_doc, list(doc_id for i in range(0, len(words_for_doc)))))\n",
        "\n",
        "# map CSV file to vocabulary-document-id pairs, flattening pairs across documents\n",
        "train_rdd = train_rdd.flatMap(map_function)\n",
        "\n",
        "# confirm mapping is as expected\n",
        "train_rdd.take(10)\n"
      ],
      "metadata": {
        "id": "95wlfHH-CAWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fe40a3-a5b8-4ed4-b15f-133d4a5e1ab8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('it', '1781133'),\n",
              " ('was', '1781133'),\n",
              " ('used', '1781133'),\n",
              " ('in', '1781133'),\n",
              " ('landing', '1781133'),\n",
              " ('craft', '1781133'),\n",
              " ('during', '1781133'),\n",
              " ('world', '1781133'),\n",
              " ('war', '1781133'),\n",
              " ('ii', '1781133')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce corpus vocabulary term and corpus document id pairs to map of vocab terms to doc id lists\n",
        "# list((term, doc_id)) -> dict({term: list(doc_id)})\n",
        "\n",
        "# program currently runs out of memory - additional review of system behavior required\n",
        "\n",
        "def reduce_function(list_of_doc_ids_for_term_instance_1, list_of_doc_ids_for_term_instance_2):\n",
        "    list_of_doc_ids_for_term_instance_1 += list_of_doc_ids_for_term_instance_2\n",
        "    return list_of_doc_ids_for_term_instance_1\n",
        "\n",
        "# train_rdd = train_rdd.reduceByKey(reduce_function).collect()\n"
      ],
      "metadata": {
        "id": "TwqBsrF-wXQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write index to CSV file on Google Drive\n",
        "\n",
        "dummy_index = {\n",
        "    'the': [1, 2, 3],\n",
        "    'quick': [1, 2, 3],\n",
        "    'brown': [1, 2, 3],\n",
        "    'fox': [1, 2, 3],\n",
        "    'jumpted': [1, 2, 3],\n",
        "    'over': [1, 2, 3],\n",
        "    'lazy': [1, 2, 3],\n",
        "    'dog': [1, 2, 3],\n",
        "}\n",
        "\n",
        "index_file = open(f\"{dataset_directory}/{training_dataset_filename}_index\", 'w+')\n",
        "csv_writer = csv.writer(index_file, dialect='unix')\n",
        "for k, v in dummy_index.items():\n",
        "    csv_writer.writerow([k, pickle.dumps(v)])\n",
        "index_file.close()\n"
      ],
      "metadata": {
        "id": "Wl_-aH3T50el"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map CSV file lines to RDD entries for the corpus inverted index\n",
        "# each line corresponds to one vocabulary term and document id list\n",
        "\n",
        "# index loading is currently non functioning due to python object serialization issues\n",
        "\n",
        "# load dataset from CSV file to RDD\n",
        "train_index_rdd = spark.textFile(f\"{dataset_directory}/{training_dataset_filename}_index\")\n",
        "train_index_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# define function for mapping CSV file lines to index entries\n",
        "def map_function(csv_file_line):\n",
        "    csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "    vocab_term = re.sub(\"^\\\"\", \"\", csv_file_line_elements[0])\n",
        "    bytes_as_string = re.sub(\"'\\\"$\", \"\", csv_file_line_elements[1])\n",
        "    bytes_as_string = re.sub(\"^b'\", \"\", bytes_as_string)\n",
        "    docs_for_term = pickle.loads(bytes_as_string.encode('utf-8'))\n",
        "    return (vocab_term, docs_for_term)\n",
        "\n",
        "# map CSV file\n",
        "# train_index_rdd = train_index_rdd.map(map_function)\n",
        "\n",
        "# confirm mapping is as expected\n",
        "# train_index_rdd.take(10)\n",
        "\n",
        "train_index_rdd = spark.parallelize(dummy_index.items())\n",
        "train_index_rdd.take(10)"
      ],
      "metadata": {
        "id": "qoj5weeNAMVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map CSV file lines to RDD entries for the wikIR queries\n",
        "# each line corresponds to one query\n",
        "\n",
        "# query loading is currently non functioning due to time constraints\n",
        "\n",
        "# load dataset from CSV file to RDD\n",
        "train_query_rdd = spark.textFile(f\"{dataset_directory}/{training_dataset_filename}_queries\")\n",
        "train_query_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# copy from above\n",
        "# def map_function(csv_file_line):\n",
        "#     csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "#     vocab_term = re.sub(\"^\\\"\", \"\", csv_file_line_elements[0])\n",
        "#     bytes_as_string = re.sub(\"'\\\"$\", \"\", csv_file_line_elements[1])\n",
        "#     bytes_as_string = re.sub(\"^b'\", \"\", bytes_as_string)\n",
        "#     docs_for_term = pickle.loads(bytes_as_string.encode('utf-8'))\n",
        "#     return (vocab_term, docs_for_term)\n",
        "\n",
        "train_query_rdd = spark.parallelize(\n",
        "    [\n",
        "        (1, 'brown fox'),\n",
        "        (2, 'lazy dog')\n",
        "    ]\n",
        ")\n",
        "train_query_rdd.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyZk9_jBCOjZ",
        "outputId": "37b2c009-7fa9-4851-d9ec-4d6457d87919"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'brown fox')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map CSV file lines to RDD entries for the wikIR qrels\n",
        "# each line corresponds to one qrel\n",
        "\n",
        "# qrel loading is currently non functioning due to time constraints\n",
        "\n",
        "# load dataset from CSV file to RDD\n",
        "train_qrels_rdd = spark.textFile(f\"{dataset_directory}/{training_dataset_filename}_qrels\")\n",
        "train_qrels_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# copy from above\n",
        "# def map_function(csv_file_line):\n",
        "#     csv_file_line_elements = csv_file_line.split('\\\",\\\"')\n",
        "#     vocab_term = re.sub(\"^\\\"\", \"\", csv_file_line_elements[0])\n",
        "#     bytes_as_string = re.sub(\"'\\\"$\", \"\", csv_file_line_elements[1])\n",
        "#     bytes_as_string = re.sub(\"^b'\", \"\", bytes_as_string)\n",
        "#     docs_for_term = pickle.loads(bytes_as_string.encode('utf-8'))\n",
        "#     return (vocab_term, docs_for_term)\n",
        "\n",
        "train_qrels_rdd = spark.parallelize(\n",
        "    [\n",
        "        (1, 1, 1, 0),\n",
        "        (1, 2, 1, 0),\n",
        "        (1, 3, 1, 0),\n",
        "        (1, 1, 1, 0),\n",
        "        (1, 2, 1, 0),\n",
        "        (1, 3, 1, 0)\n",
        "    ]\n",
        ")\n",
        "train_qrels_rdd.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcx_t89pIdaJ",
        "outputId": "5e8f8335-3b30-4bfc-fb2e-bda9a3f35f9c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1, 1, 0),\n",
              " (1, 2, 1, 0),\n",
              " (1, 3, 1, 0),\n",
              " (1, 1, 1, 0),\n",
              " (1, 2, 1, 0),\n",
              " (1, 3, 1, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation of Okapai BM25 inverted index search results\n",
        "def bm_25_evaluator(term):\n",
        "    pass\n",
        "\n",
        "def bm_25_mapping_function(query):\n",
        "    # assume ability to reference other RDDs from context of RDD mapping function\n",
        "    # if not possible, create index object that can be passed to workers\n",
        "    # docs_with_ranks = list()\n",
        "    # for term in query:\n",
        "    #   docs_with_ranks_for_term = bm_25_evaluator(term)\n",
        "    #   docs_with_ranks.expand(docs_with_ranks_for_term)\n",
        "    # return docs\n",
        "    pass\n",
        "\n",
        "train_qrels_evaluation = train_query_rdd.map(bm_25_mapping_function)"
      ],
      "metadata": {
        "id": "XWAcvco3Iv_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementatioon of negative discounted cumulative gain\n",
        "def ndcg_func():\n",
        "    pass\n",
        "\n",
        "def qrels_validation_function(qrels_evaluation):\n",
        "    # assume ability to reference other RDDs from context of RDD mapping function\n",
        "    # if not possible, create index object that can be passed to workers\n",
        "    # docs_actual = train_qrels_rdd.lookup(qrels_evaluation.query_id)\n",
        "    # accuracy_for_doc = ndcg_funct(docs_actual, qrels_evaluation.docs)\n",
        "    pass\n",
        "\n",
        "train_qrels_evaluation.map(qrels_validation_function)\n",
        "\n",
        "# method for analyzing and displaying the results of the ndcg evaluation of the BM25 search results\n",
        "def analyze_and_display_ndcg_results():\n",
        "    pass"
      ],
      "metadata": {
        "id": "WYaQHU5mJIpA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}